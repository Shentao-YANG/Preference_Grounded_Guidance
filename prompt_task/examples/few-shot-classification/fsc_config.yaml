# Few-Shot Classification Config
defaults:
 - base_fsc
 - _self_
# Data
num_shots: 16
base_path: "./data"
dataset: "sst-2"
dataset_seed: 0
# Reward
task_lm: "roberta-large"
# Single Prompt Model
prompt_length: 5
prompt_train_batch_size: 16
prompt_infer_batch_size: 1
# SQL Module
top_k: 256
# Trainer
max_train_steps: 12000
train_shuffle: false
eval_steps: 10
save_steps: 12000
learning_rate: 5e-5
random_seed: 0
output_folder: "./outputs/Exp1/sst-2/rs0ds0"         # "./outputs/Exp${EXP_IDX}/${DATASET}/rs${random_seed}ds${dataset_seed}"
# Reward function learning
reward_learning_samples: 5
reward_learning_batch_size: 64
rew_gradient_accumulation_steps: 1
rew_num_train_epochs: 100
rew_max_train_steps: 10000
agg_func: "sum"
soft_maxmin_temp: 1.
compute_rew_batch_mode: 1
rew_learning_rate: 5e-5
# Policy-reward joint learning
reward_retrain_period: 1000
early_stop_eval_period: 500
num_validation_prompts: 15
# REINFORCE param
max_entropy_coeff: 0.125
use_q_for_weight: 0       # default: do not use q for weight -> use r for weight
# reward model param
use_softplus: 1           # default: do not use softplus, use sigmoid